{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSSM 复现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据集，采用movielens测试开发模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size is 2497, test data size is 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import gen_data_set, gen_model_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data = pd.read_csvdata = pd.read_csv(\"./data/movielens_sample.txt\") # 1 继续\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\", \"genres\"]   # 对这些进行一个稀疏化\n",
    "SEQ_LEN = 50\n",
    "negsample = 10\n",
    "\n",
    "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
    "feature_max_idx = {}\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "\n",
    "# 数据分类与数据清洗\n",
    "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id')\n",
    "item_profile = data[[\"movie_id\", \"genres\"]].drop_duplicates('movie_id')\n",
    "user_profile.set_index(\"user_id\", inplace=True)\n",
    "# user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)    # 通过user来进行分类，但是似乎没有用到\n",
    "train_set, test_set = gen_data_set(data, SEQ_LEN, negsample)        # 负采样同时按照时间序列划分数据集\n",
    "train_X, train_y = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_X, test_y = gen_model_input(test_set, user_profile, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
    "from utils import SparseFeat, VarLenSparseFeat\n",
    "embedding_dim = 32\n",
    "\n",
    "# 针对每一个变量做一个属性上的定义，同时给每一个变量一个维数进行表示\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),\n",
    "                        SparseFeat(\"gender\", feature_max_idx['gender'], embedding_dim),\n",
    "                        SparseFeat(\"age\", feature_max_idx['age'], embedding_dim),\n",
    "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], embedding_dim),\n",
    "                        SparseFeat(\"zip\", feature_max_idx['zip'], embedding_dim),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_genres', feature_max_idx['genres'], embedding_dim,\n",
    "                                                    embedding_name=\"genres\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim),\n",
    "                        SparseFeat('genres', feature_max_idx['genres'], embedding_dim)\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import Movie_data\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = Movie_data(train_X, train_y, user_feature_columns, item_feature_columns)\n",
    "test_dataset = Movie_data(test_X, test_y, user_feature_columns, item_feature_columns)\n",
    "trian_len = len(train_dataset)\n",
    "test_len = len(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, trian loss:0.00400084859856043\n",
      "1, test loss:0.8552641073862711\n",
      "\n",
      "2, trian loss:0.0026010949825543525\n",
      "2, test loss:0.7100110848744711\n",
      "\n",
      "3, trian loss:0.002408641383415897\n",
      "3, test loss:0.7865094343821207\n",
      "\n",
      "4, trian loss:0.0023797642876732385\n",
      "4, test loss:1.1268568833669026\n",
      "\n",
      "5, trian loss:0.002326759639911094\n",
      "5, test loss:1.1859337488810222\n",
      "\n",
      "6, trian loss:0.002262990290659353\n",
      "6, test loss:1.3099138736724854\n",
      "\n",
      "7, trian loss:0.002239246594223157\n",
      "7, test loss:1.2887153625488281\n",
      "\n",
      "8, trian loss:0.0022461329179140487\n",
      "8, test loss:1.2638248602549236\n",
      "\n",
      "9, trian loss:0.0021830200492548\n",
      "9, test loss:1.473236878712972\n",
      "\n",
      "10, trian loss:0.0019430292275699561\n",
      "10, test loss:1.026519775390625\n",
      "\n",
      "11, trian loss:0.001160897550269705\n",
      "11, test loss:1.3074293931325276\n",
      "\n",
      "12, trian loss:0.0009125554870737999\n",
      "12, test loss:1.3415624300638835\n",
      "\n",
      "13, trian loss:0.0008287161251497975\n",
      "13, test loss:0.6752048333485922\n",
      "\n",
      "14, trian loss:0.0007820784893449326\n",
      "14, test loss:0.44756317138671875\n",
      "\n",
      "15, trian loss:0.000729028868064147\n",
      "15, test loss:0.6344024737675985\n",
      "\n",
      "16, trian loss:0.0006678836889382978\n",
      "16, test loss:0.7888909180959066\n",
      "\n",
      "17, trian loss:0.0005684401787754817\n",
      "17, test loss:0.8689110279083252\n",
      "\n",
      "18, trian loss:0.0005462729544772306\n",
      "18, test loss:0.9498865604400635\n",
      "\n",
      "19, trian loss:0.0004607942179113949\n",
      "19, test loss:1.0475277105967205\n",
      "\n",
      "20, trian loss:0.000428616949109063\n",
      "20, test loss:1.1843363444010417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import DSSM\n",
    "from torch import optim, nn\n",
    "model = DSSM(user_feature_columns, item_feature_columns)\n",
    "lr = 0.001\n",
    "l2_coff = 0.0\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coff)\n",
    "\n",
    "def ACC(y_hat, y):\n",
    "    pass\n",
    "\n",
    "num_epoch = 20\n",
    "for epoch in range(num_epoch):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_user, X_item, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X_user, X_item)\n",
    "        loss = loss_function(y_hat, y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.cpu().item()/trian_len\n",
    "    print(f'{epoch+1}, trian loss:{train_loss}')\n",
    "\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for X_user, X_item, y in test_loader:\n",
    "        y_hat = model(X_user, X_item)\n",
    "        with torch.no_grad():\n",
    "            loss = loss_function(y_hat, y.float())\n",
    "        test_loss += loss.cpu().item()/test_len\n",
    "    \n",
    "    print(f'{epoch+1}, test loss:{test_loss}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9441146428433421db0d924e28699bc81d9571c05f52cc036aaf320868297349"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
